{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./datasets/mbappe/gettyimages-1477251906-612x612.jpg\"\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(img.shape)\n",
    "print(gray.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# Extract the face\n",
    "faces = face_cascade.detectMultiScale(\n",
    "    gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    ")\n",
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the eyes\n",
    "for x, y, w, h in faces:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    roi_gray = gray[y : y + h, x : x + w]\n",
    "    roi_color = img[y : y + h, x : x + w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for ex, ey, ew, eh in eyes:\n",
    "        cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BGR image to RGB for matplotlib\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the output with matplotlib\n",
    "plt.figure()\n",
    "plt.imshow(img_rgb, cmap=\"gray\")\n",
    "# plt.imshow(img_rgb)\n",
    "# plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(roi_color, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folder_name = \"mbappe\"\n",
    "image_name = crop_images_dict[folder_name][0]\n",
    "image_path = os.path.join(\"./face_crop\", folder_name, image_name)\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert BGR to RGB\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create a dictionary with the folder name as the key and the images as as list of value\n",
    "def create_crop_images_dict(base_folder):\n",
    "    crop_images_dict = {}\n",
    "    for folder_name in os.listdir(base_folder):\n",
    "        folder_path = os.path.join(base_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            crop_images_dict[folder_name] = [\n",
    "                img for img in os.listdir(folder_path) if img.endswith(\".jpg\")\n",
    "            ]\n",
    "    return crop_images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_images_dict = create_crop_images_dict(\"./face_crop/\")\n",
    "crop_images_dict.keys(), crop_images_dict[\"oshoala\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crop_images_dict[\"oshoala\"]), len(crop_images_dict[\"messi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the key, values into X and labels\n",
    "base_path = \"./face_crop/\"\n",
    "\n",
    "X = []  # Image data\n",
    "y = []  # Labels\n",
    "\n",
    "for label, image_files in crop_images_dict.items():\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(base_path, label, image_file)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (100, 100))  # Resize to uniform size\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "\n",
    "# convert to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode y label\n",
    "# Mapping dictionary\n",
    "label_mapping = {\"oshoala\": 0, \"mbappe\": 1, \"ronaldo\": 2, \"messi\": 3, \"morgan\": 4}\n",
    "y_encode = [label_mapping[label] for label in y]\n",
    "label = np.array(y_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and Augment Data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, label, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create ImageDataGenerators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255\n",
    ")  # Only rescaling for validation data\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "validation_generator = val_datagen.flow(X_val, y_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(100, 100, 3)))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# 128 neuron hidden layer\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Dropout layer to reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "# Use softmax for multi-class classification, sigmoid for binary\n",
    "model.add(Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer, \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "# Set the number of epochs and batch size\n",
    "epochs = 50 # This maybe be finetune\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 32,  # Number of steps per epoch\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(X_val) // 32,  # Number of validation steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and Save the Model\n",
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_model.h5\",  # Filename to save model\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    verbose=1,\n",
    "    save_best_only=True,  # Only save when the monitored metric has improved\n",
    "    mode=\"min\",  # Mode 'min' for loss, 'max' for accuracy\n",
    ")\n",
    "\n",
    "# Train the model with the callback\n",
    "epochs = 50  # This maybe be finetune\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 32,  # Number of steps per epoch\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(X_val) // 32,  # Number of validation steps\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Evaluate best Model\n",
    "model = tf.keras.models.load_model(\"best_model.h5\")\n",
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our Model By Predicting New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "import tensorflow as tf\n",
    "model = model = tf.keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare th image\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "    img = img / 255.0\n",
    "    return img.reshape(1, 100, 100, 3)  # Reshape for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**label_mapping**\n",
    "* oshoala: 0\n",
    "* mbappe: 1\n",
    "* ronaldo: 2\n",
    "* messi: 3\n",
    "* morgan: 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4]),\n",
       " array([[0.00208999, 0.00261326, 0.01020569, 0.09974755, 0.88534343]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Predictions\n",
    "new_image = preprocess_image(\n",
    "    \"./face_crop/messi/face_gettyimages-82525445-612x612_2.jpg\"\n",
    ")\n",
    "prediction = model.predict(new_image)\n",
    "predicted_class = np.argmax(prediction, axis=1)  # get the max value in the predictions\n",
    "predicted_class, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
